{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #1 Part 3: Playing with Neural Networks by PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Previously in `Assignment1-1_Data_Curation.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using PyTorch.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **part 1 - 3**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n",
    "This will produce a compressed file called *[Your student number].tar.gz*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* &nbsp; 20\\*\\*-\\*\\*\\*\\*\\*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "\n",
    "First reload the data we generated in `Assignment1-1_Data_Curation.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [],
   "source": [
    "pickle_file = 'data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- unnormalize data\n",
    "- data as a flat matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset):\n",
    "    dataset = dataset * 255.0 + 255.0/2\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = reformat(train_dataset)\n",
    "valid_dataset = reformat(valid_dataset)\n",
    "test_dataset = reformat(test_dataset)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "## PyTorch tutorial: Fully Connected Network\n",
    "\n",
    "We're first going to train a **fully connected network** with *1 hidden layer* with *1024 units* using stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first, define NotMNIST dataset class.  \n",
    "- dataset class inherits torch.utils.data.Dataset class  \n",
    "- every dataset class should define \\_\\_len\\_\\_() and \\_\\_getitem\\_\\_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotMNIST(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data_idx = self.data[idx]\n",
    "        label_idx = self.label[idx]\n",
    "        return data_idx, label_idx\n",
    "\n",
    "    \n",
    "notmnist_train = NotMNIST(train_dataset, train_labels)\n",
    "notmnist_valid = NotMNIST(valid_dataset, valid_labels)\n",
    "notmnist_test = NotMNIST(test_dataset, test_labels)\n",
    "\n",
    "print('training set length: ', len(notmnist_train))\n",
    "print('validation set length: ', len(notmnist_valid))\n",
    "print('test set length: ', len(notmnist_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, make dataloader using NotMNIST dataset objects  \n",
    "Note that torch.utils.data.DataLoader is a subclass of Iterable, which means it can be used with 'for' statement (for more detailed explanation of Iterable, refer to https://shoark7.github.io/programming/python/iterable-iterator-generator-in-python)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(dataset=notmnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(dataset=notmnist_valid, batch_size=len(notmnist_valid), shuffle=True)\n",
    "test_loader = DataLoader(dataset=notmnist_test, batch_size=len(notmnist_test), shuffle=True)\n",
    "\n",
    "from collections.abc import Iterable\n",
    "print(issubclass(DataLoader, Iterable))\n",
    "\n",
    "print('train loader length: ', len(train_loader)) # same as len(dataset) // batch_size\n",
    "print('valid loader length: ', len(valid_loader))\n",
    "print('test loader length: ', len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Naive Linear model\n",
    "- model should inherit nn.Module\n",
    "- implement feed forward by overriding **forward** method of nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "class NaiveLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(NaiveLinear, self).__init__()\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.bias = Parameter(torch.Tensor(out_features))\n",
    "        torch.nn.init.uniform_(self.weight, -1.0, 1.0)\n",
    "        torch.nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return torch.matmul(input, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, nn_hidden, num_labels):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = NaiveLinear(in_features, nn_hidden)\n",
    "        self.fc2 = NaiveLinear(nn_hidden, num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_hidden = 1024\n",
    "\n",
    "model = Model(image_size*image_size, nn_hidden, num_labels)\n",
    "\n",
    "# move model to GPU\n",
    "model.cuda()\n",
    "# print model, initialized weight, grad buffer\n",
    "print(model)\n",
    "print(model.fc1.weight.data)\n",
    "print(model.fc1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "log_step = 1000\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    logits = logits.cpu().detach().numpy()\n",
    "    labels = labels.cpu().detach().numpy()\n",
    "    \n",
    "    return (100.0 * np.sum(np.equal(np.argmax(logits, 1), labels))\n",
    "          / logits.shape[0])\n",
    "\n",
    "\n",
    "# train_model\n",
    "def train(model, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        images_flatten, labels = data[0].cuda(), data[1].long().cuda()\n",
    "        logits = model(images_flatten)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx % log_step) == log_step-1:\n",
    "            print(f'epoch: {epoch+1} [{idx + 1} / {len(train_loader)}]\\t train_loss: {loss.item():.3f}\\t train_accuracy: {accuracy(logits, labels):.1f}\\n')\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "def evaluate(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(test_loader):\n",
    "            test_images_flatten, test_labels = data[0].cuda(), data[1].cuda()\n",
    "            test_logits = model(test_images_flatten)\n",
    "\n",
    "        print(f'accuracy: {accuracy(test_logits, test_labels):.1f}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(model, train_loader, optimizer, criterion, epoch)\n",
    "    print('-------- validation --------')\n",
    "    evaluate(model, valid_loader)\n",
    "\n",
    "            \n",
    "print('-------- test ---------')\n",
    "evaluate(model, test_loader)\n",
    "\n",
    "    \n",
    "# save model\n",
    "torch.save(model.state_dict(), './model_checkpoints/naive_model_final.pt')\n",
    "print('naive model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, you have built the model in a naive way. However, PyTorch provides a linear module named nn.Linear for your convenience. \n",
    "\n",
    "From now on, build the same model as above using layers module.\n",
    "\n",
    "You can also build model using nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layer = nn.Sequential(\n",
    "            # neural network using nn.Linear\n",
    "            nn.Linear(image_size * image_size, nn_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(nn_hidden, num_labels),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "            )\n",
    "\n",
    "model_layer.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_layer = nn.NLLLoss()\n",
    "optimizer_layer = optim.SGD(model_layer.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(model_layer, train_loader, optimizer_layer, criterion_layer, epoch)\n",
    "    print('-------- validation --------')\n",
    "    evaluate(model_layer, valid_loader)\n",
    "\n",
    "            \n",
    "print('-------- test ---------')\n",
    "evaluate(model_layer, test_loader)\n",
    "\n",
    "    \n",
    "# save model\n",
    "torch.save(model_layer.state_dict(), './model_checkpoints/layer_model_final.pt')\n",
    "print('layer_model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 1\n",
    "-------\n",
    "\n",
    "**Describe below** why there is a difference in an accuracy between the model using nn.Linear and the model which is built in a naive way. **explain simply**  \n",
    "You can refer to PyTorch documentation(https://pytorch.org/docs/stable/index.html) to check nn.Linear() implementation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">\n",
    "/Assignment1_sol/\n",
    "Name\n",
    "Last Modified\n",
    "\n",
    "---\n",
    "Problem 2\n",
    "-------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! (It doesn't matter whether you implement it in a naive way or using layer module. HOWEVER, you CANNOT use other type of layers such as conv.) \n",
    "\n",
    "The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.kr/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595). You may use techniques below.\n",
    "\n",
    "1. Experiment with different hyperparameters: epochs, learning rate, etc.\n",
    "2. We used a fixed learning rate epsilon for gradient descent. Implement an annealing schedule for the gradient descent learning rate ([more info](http://cs231n.github.io/neural-networks-3/#anneal)). *Hint*. Try using `torch.optim.lr_scheduler.ExponentialLR()`.    \n",
    "3. We used a tanh activation function for our hidden layer. Experiment with other activation functions included in PyTorch.\n",
    "4. Extend the network to multiple hidden layers. Experiment with the layer sizes. Adding another hidden layer means you will need to adjust the code. \n",
    "5. Introduce and tune regularization method (e.g. L2 regularization) for your model. Remeber that L2 amounts to adding a penalty on the norm of the weights to the loss. The right amount of regularization should imporve your validation / test accuracy.\n",
    "\n",
    "\n",
    "**Evaluation:** You will get full credit if your best test accuracy exceeds <font color=red>$93\\%$</font>. Save your best perfoming model using saver.  \n",
    "**<font color=red>Save your model in directory ./model_checkpoints</font>** (Refer to the cell above)  \n",
    "**<font color=red>Please follow format as problem2_(Student Number)</font>** (e.g. set path as './model_checkpoints/problem2_2020-23456')\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\"\"\" TODO \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
