{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #4 Implementing Conditional Generative Adversarial Nets - part3 Labeled MNIST\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Chaehun Shin, September 2020\n",
    "\n",
    "In this notebook, you will learn how to implement conditional Genverative Adversarial Nets (cGANs) <br>\n",
    "The goal here is to build GANs that draw hand-written digit image given its label. You can draw the digit 0~9 as you give an input at the end of training.<br>\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all parts**, run the *CollectSubmission.sh* script with your **Student_ID** as input argument. <br>\n",
    "This will produce a zipped file called *[Your Student_ID].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* &nbsp; Student_ID#)\n",
    "\n",
    "### Some helpful tutorials and references for assignment #4-3:\n",
    "- [1] Pytorch official tutorials. [[link]](https://pytorch.org/tutorials/)\n",
    "- [2] Stanford CS231n lectures. [[link]](http://cs231n.stanford.edu/)\n",
    "- [3] Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems. 2014.\n",
    "- [4] Mirza, Mehdi, and Simon Osindero. \"Conditional generative adversarial nets.\" arXiv preprint arXiv:1411.1784 (2014).\n",
    "- [5] Radford, Alec, Luke Metz, and Soumith Chintala. \"Unsupervised representation learning with deep convolutional generative adversarial networks.\" arXiv preprint arXiv:1511.06434 (2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Download and load MNIST datasets\n",
    "The MNIST datasets will be downloaded into the 'data/mnist' directory. If you want to change the directory the data is saved in, change 'mnist_data_dir' with where you want. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccw/anaconda3/envs/deep-learning-20/lib/python3.8/site-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/ccw/anaconda3/envs/deep-learning-20/lib/python3.8/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as T\n",
    "\n",
    "mnist_data_dir = './data/mnist'\n",
    "dataset = MNIST(root=mnist_data_dir,\n",
    "               transform=T.ToTensor(), train=True, download=True)\n",
    "print(dataset.train_data.shape)\n",
    "print(dataset.train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\"></a> 1. Building a network\n",
    "\n",
    "In this section, you will implement neural networks for <br>\n",
    "(1) generator model to draw a digit corresponding to given label<br>\n",
    "(2) discriminator model to distinguish real images from generated images according to given labels.<br>\n",
    "You can reuse your code in part2 and improve it. \n",
    "Just write the code in whatever way you find most clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time for a generator model.\n",
    "You can change anything including the argument if you need. Feel Free to change it and improve it.<br>\n",
    "**(You should output the image as a range (0, 1) with Sigmoid function because we normalize the real images as a range (0, 1))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conditional_Generator(nn.Module):\n",
    "    def __init__(self, condition_dim=10, latent_dim=30, img_dim=1):\n",
    "        super().__init__()\n",
    "        ################ ToDo ################\n",
    "        self.label_emb = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 784),\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, condition):\n",
    "        ################ ToDo ################\n",
    "        z = z.view(z.size(0), 100)\n",
    "        c = self.label_emb(condition)\n",
    "        print(c.shape)\n",
    "        print(condition.shape)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        out = F.sigmoid(self.model(x)).view(x.size(0), 1, 28, 28)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time for a discriminative model. Again, you can change anything if you need.\n",
    "**(You should output the probability of whether the input image of discriminator is real or not. It means that you use the Sigmoid function at the last layer to make the value being in range (0, 1))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conditional_Discriminator(nn.Module):\n",
    "    def __init__(self, condition_dim=10, img_dim=1):\n",
    "        super().__init__()\n",
    "        ################ ToDo ################\n",
    "        self.label_emb = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, img, condition):\n",
    "        ################ ToDo ################\n",
    "        img = img.view(img.size(0), 784)\n",
    "        c = self.label_emb(condition)\n",
    "        print(c)\n",
    "        x = torch.cat([img, c], 1)\n",
    "        out = self.model(x)\n",
    "        return out.squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"2\"></a> 2. Build a main part and train\n",
    "\n",
    "In this section, you will implement the main part (define criterion variable and D_loss/G_loss to train in TODO parts, you can also use the criterion variable).\n",
    "Feel free to set the hyperparmeters and fill in the main part.\n",
    "When you are done, run the following to check your implementations.\n",
    "\n",
    "Your goal is to **generate 10 row and 10 column images(100 total)**.<br>\n",
    "**Each column should correspond to each label as an order**.<br>\n",
    "You must show **at least three generated images** (At the beginning of , in the midway of, at the end of training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter setting\n",
    "img_dim=1\n",
    "img_size = 28\n",
    "latent_dim = 100\n",
    "condition_dim=10\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "total_iter = 50000\n",
    "\n",
    "log_freq = 10\n",
    "viz_freq = 200\n",
    "\n",
    "gen_num_samples = 100\n",
    "gen_conditions = torch.tensor(list(range(10))*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10, 10])\n",
      "torch.Size([128, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-eaeb87db17bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mfake_conditions_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_conditions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mfake_conditions_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_conditions_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mfake_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_conditions_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-20/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-7b371a0eca72>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, condition)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dataloader_iter = iter(dataloader)\n",
    "\n",
    "netG = Conditional_Generator(condition_dim, latent_dim, img_dim).to(device)\n",
    "netD = Conditional_Discriminator(condition_dim, img_dim).to(device)\n",
    "\n",
    "optimG = torch.optim.Adam(netG.parameters(), learning_rate)\n",
    "optimD = torch.optim.Adam(netD.parameters(), learning_rate)\n",
    "\n",
    "real_labels = torch.ones(batch_size).to(device)\n",
    "fake_labels = torch.zeros(batch_size).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for it in range(total_iter):\n",
    "    # train Discriminator\n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            real_imgs, real_conditions = next(dataloader_iter)\n",
    "        except:\n",
    "            dataloader_iter = iter(dataloader)\n",
    "            real_imgs, real_conditions = next(dataloader_iter)\n",
    "\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        real_conditions_onehot = torch.zeros((batch_size, condition_dim))\n",
    "        real_conditions_onehot.scatter_(1, real_conditions.long().unsqueeze(-1), 1.)\n",
    "        real_conditions_onehot = real_conditions_onehot.to(device).long()\n",
    "        \n",
    "        z = torch.randn((batch_size, latent_dim)).to(device)\n",
    "        fake_conditions = torch.randint(0, condition_dim, (batch_size,))\n",
    "        fake_conditions_onehot = torch.zeros((batch_size, condition_dim))\n",
    "        fake_conditions_onehot.scatter_(1, fake_conditions.long().unsqueeze(-1), 1.)\n",
    "        fake_conditions_onehot = fake_conditions_onehot.to(device).long()\n",
    "        fake_imgs = netG(z, fake_conditions_onehot).detach()\n",
    "        \n",
    "        \n",
    "        real_probs = netD(real_imgs, real_conditions_onehot).squeeze()\n",
    "        fake_probs = netD(fake_imgs, fake_conditions_onehot).squeeze()\n",
    "        \n",
    "        ################ ToDo ################\n",
    "        real_loss = criterion(real_probs, real_labels)\n",
    "        fake_loss = criterion(fake_probs, fake_labels)\n",
    "        D_loss = real_loss + fake_loss\n",
    "        \n",
    "        optimD.zero_grad()\n",
    "        D_loss.backward()\n",
    "        optimD.step()\n",
    "      \n",
    "    # train the Generator\n",
    "    z = torch.randn((batch_size, latent_dim)).to(device)\n",
    "    fake_conditions = torch.randint(0, condition_dim, (batch_size,))\n",
    "    fake_conditions_onehot = torch.zeros((batch_size, condition_dim))\n",
    "    fake_conditions_onehot.scatter_(1, fake_conditions.long().unsqueeze(-1), 1.)\n",
    "    fake_conditions_onehot = fake_conditions_onehot.to(device)\n",
    "    fake_imgs = netG(z, fake_conditions_onehot)\n",
    "        \n",
    "    fake_probs = netD(fake_imgs, fake_conditions_onehot)\n",
    "    \n",
    "    ################ ToDo ################\n",
    "    G_loss = criterion(fake_probs, real_labels)\n",
    "    \n",
    "    optimG.zero_grad()\n",
    "    G_loss.backward()\n",
    "    optimG.step()\n",
    " \n",
    "    \n",
    "    if (it+1) % log_freq == 0:\n",
    "        print(\"Iter: %05d/%d, Gen loss: %.4f, Dis loss: %.4f\"%(it+1, total_iter,\n",
    "                                                              D_loss.data.item(),\n",
    "                                                              G_loss.data.item()))\n",
    "    if (it+1) % viz_freq == 0:\n",
    "        z = torch.randn((100, latent_dim)).to(device)\n",
    "        gen_conditions_onehot = torch.zeros((100, condition_dim))\n",
    "        gen_conditions_onehot.scatter_(1, gen_conditions.long().unsqueeze(-1), 1.)\n",
    "        gen_conditions_onehot = gen_conditions_onehot.to(device)\n",
    "        with torch.no_grad():\n",
    "            gen_imgs = netG(z, gen_conditions_onehot)\n",
    "        \n",
    "        gen_imgs = make_grid(gen_imgs, nrow=10).permute(1, 2, 0).cpu().detach().numpy()\n",
    "        plt.imshow(gen_imgs)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning-20] *",
   "language": "python",
   "name": "conda-env-deep-learning-20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
