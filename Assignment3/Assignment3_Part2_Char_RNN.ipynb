{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 2: Language Modeling with CharRNN\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Sangil Lee, October 2018, modified by Jungbeom Lee, October 2020.\n",
    "\n",
    "This is  a basic character-level RNN to classify words.\n",
    "\n",
    "A character-level RNN reads words as a series of characters - outputting a prediction and ?hidden state? at each step, feeding its previous hidden state into each next step. We take the final prediction to be the output, i.e. which class the word belongs to.\n",
    "\n",
    "Specifically, we will train on a few thousand surnames from 18 languages of origin, and predict which language a name is from based on the spelling:\n",
    "\n",
    "\n",
    "Original blog post & code:\n",
    "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "\n",
    "\n",
    "This iPython notebook is basically a copypasta of this repo.\n",
    "\n",
    "That said, you are allowed to copy paste the codes from the original repo.\n",
    "HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n",
    "You will learn a lot while wrapping around your head during the implementation. And you will understand nuts and bolts of RNNs more clearly in a code level.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1-3**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your student number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* team_#)\n",
    "\n",
    "### Classifying words with character-level RNN (30 points)\n",
    "\n",
    "\n",
    "1. Successful training through implementing code that works. You will need to implement the codes in char_rnn.py.  (15 points)\n",
    "\n",
    "\n",
    "2. After training, the final accuracy must be <font color=red> above 65% </font> (please see the last code block). We don't split the data into train-valid-test. Don't forget to <font color=red> NOT clear the outputs of all the code blocks! (15 points)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now proceed to the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('lus√†rski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "\n",
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names to Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings for training and inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed parameters, should not be changed.\n",
    "N_CATEGORIES = len(all_categories)\n",
    "N_LETTERS = len(all_letters)\n",
    "N_EVAL_SAMPLES_PER_CATEGORY = 10\n",
    "\n",
    "# Adjustable parameters. You can change these parameters freely.\n",
    "\n",
    "N_HIDDEN = 128\n",
    "LEARNING_RATE = 0.005\n",
    "N_ITERS = 500000\n",
    "print_every = 50000\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = Vietnamese / line = Luu\n",
      "category = Greek / line = Papadelias\n",
      "category = Portuguese / line = Gouveia\n",
      "category = Portuguese / line = Simoes\n",
      "category = German / line = Drechsler\n",
      "category = Japanese / line = Fujimoto\n",
      "category = Scottish / line = Dickson\n",
      "category = English / line = Wyer\n",
      "category = Japanese / line = Kunomasu\n",
      "category = Vietnamese / line = Thao\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "def load_train_example(n_per_cls):\n",
    "    category_tensors, line_tensors = [], []\n",
    "    for category in all_categories:\n",
    "        for line in category_lines[category][:n_per_cls]:\n",
    "            category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "            line_tensor = lineToTensor(line)\n",
    "            category_tensors.append(category_tensor)\n",
    "            line_tensors.append(line_tensor)\n",
    "    \n",
    "    return category_tensors, line_tensors\n",
    "    \n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)\n",
    "\n",
    "category_tensors, line_tensors = load_train_example(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Should NOT change this code block\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = torch.zeros(1, N_HIDDEN).cuda()\n",
    "    category_tensor = category_tensor.cuda()\n",
    "    line_tensor = line_tensor.cuda()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    output = rnn(line_tensor, hidden)\n",
    "    # print(output.shape)\n",
    "    # print(category_tensor.shape)\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-LEARNING_RATE)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from char_rnn import RNN\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "50000 10% (0.5573934316635132) Yim / Korean correct! 0.005\n",
      "100000 20% (0.7309585809707642) Callaghan / Irish correct! 0.005\n",
      "150000 30% (0.806131899356842) Pho / Vietnamese correct! 0.005\n",
      "200000 40% (0.4943086504936218) Ciardha / Irish correct! 0.005\n",
      "250000 50% (0.005288064945489168) Sokolowski / Polish correct! 0.005\n",
      "300000 60% (0.09815438836812973) Chavez / Spanish correct! 0.005\n",
      "350000 70% (0.193563774228096) Ingersleben / German correct! 0.005\n",
      "400000 80% (3.4793293476104736) De angelis / Greek ? (Italian) 0.005\n",
      "450000 90% (3.8885960578918457) Malafa / Irish ? (Czech) 0.005\n",
      "500000 100% (2.86383318901062) Stanzel / Polish ? (Czech) 0.005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_CATEGORIES).cuda()\n",
    "rnn.train()\n",
    "criterion = nn.NLLLoss()\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "for iter in range(1, N_ITERS + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    '''\n",
    "    print(category)\n",
    "    print(line)\n",
    "    print(category_tensor)\n",
    "    print(line_tensor)\n",
    "    '''\n",
    "    \n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = 'correct!' if guess == category else '? (%s)' % category\n",
    "        print('%d %d%% (%s) %s / %s %s %s' % (iter, iter / N_ITERS * 100, loss, line, guess, correct, LEARNING_RATE))\n",
    "torch.save(rnn.state_dict(), 'models/RNN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval with 180 samples\n",
      "1.075471538827373\n",
      "0.6555555555555556\n"
     ]
    }
   ],
   "source": [
    "## Should NOT change this code block\n",
    "category_tensors, line_tensors = load_train_example(N_EVAL_SAMPLES_PER_CATEGORY)\n",
    "\n",
    "total_loss = 0\n",
    "n_samples = 0\n",
    "n_correct = 0\n",
    "for idx in range(len(category_tensors)):\n",
    "    n_samples += 1\n",
    "    category_tensor, line_tensor = category_tensors[idx].cuda(), line_tensors[idx].cuda()\n",
    "    hidden = torch.zeros(1, N_HIDDEN).cuda()\n",
    "    \n",
    "    output = rnn(line_tensor, hidden)\n",
    "\n",
    "    \n",
    "    loss = criterion(output, category_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "#     print(guess_i, category_tensor)\n",
    "    if guess_i == category_tensor[0].data.cpu().numpy():\n",
    "        n_correct += 1\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "print(\"eval with %d samples\" % n_samples)\n",
    "print(total_loss / n_samples)\n",
    "print(n_correct / n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval with 180 samples\n",
      "1.075471538827373\n",
      "0.6555555555555556\n"
     ]
    }
   ],
   "source": [
    "# check your saved checkpoint again\n",
    "\n",
    "rnn = RNN(N_LETTERS, N_HIDDEN, N_CATEGORIES).cuda()\n",
    "rnn.eval()\n",
    "rnn.load_state_dict(torch.load('models/RNN.pth'), strict=True)\n",
    "\n",
    "category_tensors, line_tensors = load_train_example(N_EVAL_SAMPLES_PER_CATEGORY)\n",
    "\n",
    "total_loss = 0\n",
    "n_samples = 0\n",
    "n_correct = 0\n",
    "for idx in range(len(category_tensors)):\n",
    "    n_samples += 1\n",
    "    category_tensor, line_tensor = category_tensors[idx].cuda(), line_tensors[idx].cuda()\n",
    "    hidden = torch.zeros(1, N_HIDDEN).cuda()\n",
    "    \n",
    "    output = rnn(line_tensor, hidden)\n",
    "\n",
    "    \n",
    "    loss = criterion(output, category_tensor)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "#     print(guess_i, category_tensor)\n",
    "    if guess_i == category_tensor[0].data.cpu().numpy():\n",
    "        n_correct += 1\n",
    "    total_loss += loss.item()\n",
    "    \n",
    "print(\"eval with %d samples\" % n_samples)\n",
    "print(total_loss / n_samples)\n",
    "print(n_correct / n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning-20] *",
   "language": "python",
   "name": "conda-env-deep-learning-20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
