{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 3: Transformer\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Jeonghee Jo, October 2019, and modified by Jungbeom Lee, October 2020.\n",
    "\n",
    "This is about Transformer (Vaswani et al., 2017).\n",
    "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n",
    "\n",
    "Original blog post & code:\n",
    "https://github.com/Kyubyong/transformer (Tensorflow)\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html (PyTorch)\n",
    "\n",
    "\n",
    "That said, you are allowed to copy paste the codes from the original repo.\n",
    "HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n",
    "You will learn a lot while wrapping around your head during the implementation. And you will understand nuts and bolts of Transformers more clearly in a code level.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1-3**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your student number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* 20xx-xxxxx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The Grading is as follows:\n",
    "\n",
    "This assignment is an on/off one: just make this notebook **\"work\"** without problem by: \n",
    "\n",
    "1. Train your model using at least <font color=red> 12 different hyperparameter set </font>. Report performance results computed in the last code block <font color=red> for corresponding each hyperparameter set </font>. Plus, <font color=red> submit the one checkpoint file </font> of your best model. \n",
    "\n",
    "2. Please provide the analysis of changed hyper-parameters. (10 points)\n",
    "\n",
    "The details are described in <font color=red>**transformer_modules.py**</font>. (There is nothing to implement in this notebook.)\n",
    "\n",
    "\n",
    "Now proceed to the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change hyper-parameters in this code block!\n",
    "\n",
    "\n",
    "emsize = 256 # embedding dimension\n",
    "nhid = 128 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.5 # the dropout value\n",
    "batch_size = 32\n",
    "eval_batch_size = 16\n",
    "epochs = 10 # The number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# pip install torchtext==0.6.0\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_modules import *\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccw/anaconda3/envs/deep-learning-20/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:350: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1863 batches | lr 5.00 | ms/batch 14.33 | loss  7.92 | ppl  2760.07\n",
      "| epoch   1 |   400/ 1863 batches | lr 5.00 | ms/batch 13.43 | loss  6.86 | ppl   956.05\n",
      "| epoch   1 |   600/ 1863 batches | lr 5.00 | ms/batch 13.28 | loss  6.54 | ppl   695.26\n",
      "| epoch   1 |   800/ 1863 batches | lr 5.00 | ms/batch 13.72 | loss  6.46 | ppl   638.69\n",
      "| epoch   1 |  1000/ 1863 batches | lr 5.00 | ms/batch 13.32 | loss  6.40 | ppl   604.32\n",
      "| epoch   1 |  1200/ 1863 batches | lr 5.00 | ms/batch 13.30 | loss  6.31 | ppl   551.40\n",
      "| epoch   1 |  1400/ 1863 batches | lr 5.00 | ms/batch 13.32 | loss  6.26 | ppl   521.18\n",
      "| epoch   1 |  1600/ 1863 batches | lr 5.00 | ms/batch 13.32 | loss  6.22 | ppl   500.32\n",
      "| epoch   1 |  1800/ 1863 batches | lr 5.00 | ms/batch 13.30 | loss  6.21 | ppl   496.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 26.27s | valid loss  5.98 | valid ppl   396.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1863 batches | lr 4.51 | ms/batch 13.39 | loss  6.15 | ppl   468.21\n",
      "| epoch   2 |   400/ 1863 batches | lr 4.51 | ms/batch 13.38 | loss  6.12 | ppl   455.14\n",
      "| epoch   2 |   600/ 1863 batches | lr 4.51 | ms/batch 13.60 | loss  6.06 | ppl   426.92\n",
      "| epoch   2 |   800/ 1863 batches | lr 4.51 | ms/batch 13.31 | loss  6.06 | ppl   430.43\n",
      "| epoch   2 |  1000/ 1863 batches | lr 4.51 | ms/batch 13.31 | loss  6.09 | ppl   440.61\n",
      "| epoch   2 |  1200/ 1863 batches | lr 4.51 | ms/batch 13.37 | loss  6.07 | ppl   431.13\n",
      "| epoch   2 |  1400/ 1863 batches | lr 4.51 | ms/batch 13.34 | loss  6.02 | ppl   410.20\n",
      "| epoch   2 |  1600/ 1863 batches | lr 4.51 | ms/batch 13.34 | loss  5.99 | ppl   398.98\n",
      "| epoch   2 |  1800/ 1863 batches | lr 4.51 | ms/batch 13.38 | loss  6.02 | ppl   412.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 26.11s | valid loss  5.86 | valid ppl   349.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1863 batches | lr 4.29 | ms/batch 13.54 | loss  6.01 | ppl   405.79\n",
      "| epoch   3 |   400/ 1863 batches | lr 4.29 | ms/batch 13.67 | loss  5.99 | ppl   399.19\n",
      "| epoch   3 |   600/ 1863 batches | lr 4.29 | ms/batch 13.41 | loss  5.92 | ppl   372.82\n",
      "| epoch   3 |   800/ 1863 batches | lr 4.29 | ms/batch 13.44 | loss  5.96 | ppl   385.92\n",
      "| epoch   3 |  1000/ 1863 batches | lr 4.29 | ms/batch 13.53 | loss  5.96 | ppl   386.83\n",
      "| epoch   3 |  1200/ 1863 batches | lr 4.29 | ms/batch 13.48 | loss  5.95 | ppl   384.65\n",
      "| epoch   3 |  1400/ 1863 batches | lr 4.29 | ms/batch 13.47 | loss  5.89 | ppl   362.11\n",
      "| epoch   3 |  1600/ 1863 batches | lr 4.29 | ms/batch 13.50 | loss  5.88 | ppl   358.92\n",
      "| epoch   3 |  1800/ 1863 batches | lr 4.29 | ms/batch 13.41 | loss  5.91 | ppl   367.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 26.31s | valid loss  5.82 | valid ppl   335.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 1863 batches | lr 4.07 | ms/batch 13.48 | loss  5.90 | ppl   365.81\n",
      "| epoch   4 |   400/ 1863 batches | lr 4.07 | ms/batch 13.44 | loss  5.87 | ppl   353.48\n",
      "| epoch   4 |   600/ 1863 batches | lr 4.07 | ms/batch 13.50 | loss  5.83 | ppl   339.01\n",
      "| epoch   4 |   800/ 1863 batches | lr 4.07 | ms/batch 13.51 | loss  5.85 | ppl   348.84\n",
      "| epoch   4 |  1000/ 1863 batches | lr 4.07 | ms/batch 13.55 | loss  5.87 | ppl   352.68\n",
      "| epoch   4 |  1200/ 1863 batches | lr 4.07 | ms/batch 13.92 | loss  5.85 | ppl   348.12\n",
      "| epoch   4 |  1400/ 1863 batches | lr 4.07 | ms/batch 13.54 | loss  5.80 | ppl   329.83\n",
      "| epoch   4 |  1600/ 1863 batches | lr 4.07 | ms/batch 13.57 | loss  5.79 | ppl   327.61\n",
      "| epoch   4 |  1800/ 1863 batches | lr 4.07 | ms/batch 13.58 | loss  5.83 | ppl   339.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 26.48s | valid loss  5.76 | valid ppl   317.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 1863 batches | lr 3.87 | ms/batch 13.65 | loss  5.82 | ppl   337.24\n",
      "| epoch   5 |   400/ 1863 batches | lr 3.87 | ms/batch 13.56 | loss  5.79 | ppl   328.07\n",
      "| epoch   5 |   600/ 1863 batches | lr 3.87 | ms/batch 13.61 | loss  5.75 | ppl   312.87\n",
      "| epoch   5 |   800/ 1863 batches | lr 3.87 | ms/batch 13.60 | loss  5.77 | ppl   320.65\n",
      "| epoch   5 |  1000/ 1863 batches | lr 3.87 | ms/batch 13.50 | loss  5.77 | ppl   321.89\n",
      "| epoch   5 |  1200/ 1863 batches | lr 3.87 | ms/batch 13.55 | loss  5.78 | ppl   322.60\n",
      "| epoch   5 |  1400/ 1863 batches | lr 3.87 | ms/batch 13.62 | loss  5.71 | ppl   301.25\n",
      "| epoch   5 |  1600/ 1863 batches | lr 3.87 | ms/batch 13.62 | loss  5.71 | ppl   302.26\n",
      "| epoch   5 |  1800/ 1863 batches | lr 3.87 | ms/batch 13.61 | loss  5.74 | ppl   311.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 26.52s | valid loss  5.77 | valid ppl   321.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 1863 batches | lr 3.68 | ms/batch 13.62 | loss  5.74 | ppl   309.82\n",
      "| epoch   6 |   400/ 1863 batches | lr 3.68 | ms/batch 14.34 | loss  5.70 | ppl   300.19\n",
      "| epoch   6 |   600/ 1863 batches | lr 3.68 | ms/batch 14.31 | loss  5.67 | ppl   289.84\n",
      "| epoch   6 |   800/ 1863 batches | lr 3.68 | ms/batch 14.19 | loss  5.70 | ppl   297.77\n",
      "| epoch   6 |  1000/ 1863 batches | lr 3.68 | ms/batch 14.15 | loss  5.70 | ppl   298.17\n",
      "| epoch   6 |  1200/ 1863 batches | lr 3.68 | ms/batch 14.49 | loss  5.71 | ppl   300.57\n",
      "| epoch   6 |  1400/ 1863 batches | lr 3.68 | ms/batch 14.17 | loss  5.64 | ppl   282.45\n",
      "| epoch   6 |  1600/ 1863 batches | lr 3.68 | ms/batch 14.33 | loss  5.63 | ppl   279.98\n",
      "| epoch   6 |  1800/ 1863 batches | lr 3.68 | ms/batch 13.73 | loss  5.67 | ppl   290.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 27.50s | valid loss  5.74 | valid ppl   310.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 1863 batches | lr 3.49 | ms/batch 13.81 | loss  5.67 | ppl   289.16\n",
      "| epoch   7 |   400/ 1863 batches | lr 3.49 | ms/batch 13.68 | loss  5.64 | ppl   280.62\n",
      "| epoch   7 |   600/ 1863 batches | lr 3.49 | ms/batch 13.72 | loss  5.61 | ppl   271.94\n",
      "| epoch   7 |   800/ 1863 batches | lr 3.49 | ms/batch 14.09 | loss  5.63 | ppl   279.36\n",
      "| epoch   7 |  1000/ 1863 batches | lr 3.49 | ms/batch 13.73 | loss  5.65 | ppl   283.02\n",
      "| epoch   7 |  1200/ 1863 batches | lr 3.49 | ms/batch 13.70 | loss  5.64 | ppl   280.86\n",
      "| epoch   7 |  1400/ 1863 batches | lr 3.49 | ms/batch 13.81 | loss  5.57 | ppl   263.48\n",
      "| epoch   7 |  1600/ 1863 batches | lr 3.49 | ms/batch 13.74 | loss  5.58 | ppl   264.90\n",
      "| epoch   7 |  1800/ 1863 batches | lr 3.49 | ms/batch 13.70 | loss  5.61 | ppl   273.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 26.86s | valid loss  5.68 | valid ppl   293.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 1863 batches | lr 3.32 | ms/batch 13.92 | loss  5.60 | ppl   270.25\n",
      "| epoch   8 |   400/ 1863 batches | lr 3.32 | ms/batch 13.80 | loss  5.56 | ppl   260.85\n",
      "| epoch   8 |   600/ 1863 batches | lr 3.32 | ms/batch 14.26 | loss  5.55 | ppl   256.95\n",
      "| epoch   8 |   800/ 1863 batches | lr 3.32 | ms/batch 14.96 | loss  5.57 | ppl   261.95\n",
      "| epoch   8 |  1000/ 1863 batches | lr 3.32 | ms/batch 14.95 | loss  5.58 | ppl   264.64\n",
      "| epoch   8 |  1200/ 1863 batches | lr 3.32 | ms/batch 13.90 | loss  5.57 | ppl   263.51\n",
      "| epoch   8 |  1400/ 1863 batches | lr 3.32 | ms/batch 13.83 | loss  5.51 | ppl   247.22\n",
      "| epoch   8 |  1600/ 1863 batches | lr 3.32 | ms/batch 13.87 | loss  5.51 | ppl   247.99\n",
      "| epoch   8 |  1800/ 1863 batches | lr 3.32 | ms/batch 13.78 | loss  5.55 | ppl   257.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 27.61s | valid loss  5.72 | valid ppl   303.50\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   9 |   200/ 1863 batches | lr 3.15 | ms/batch 14.55 | loss  5.54 | ppl   254.63\n",
      "| epoch   9 |   400/ 1863 batches | lr 3.15 | ms/batch 13.83 | loss  5.52 | ppl   248.57\n",
      "| epoch   9 |   600/ 1863 batches | lr 3.15 | ms/batch 13.81 | loss  5.49 | ppl   243.46\n",
      "| epoch   9 |   800/ 1863 batches | lr 3.15 | ms/batch 13.81 | loss  5.52 | ppl   249.54\n",
      "| epoch   9 |  1000/ 1863 batches | lr 3.15 | ms/batch 13.77 | loss  5.53 | ppl   252.47\n",
      "| epoch   9 |  1200/ 1863 batches | lr 3.15 | ms/batch 13.81 | loss  5.53 | ppl   251.57\n",
      "| epoch   9 |  1400/ 1863 batches | lr 3.15 | ms/batch 13.80 | loss  5.46 | ppl   235.45\n",
      "| epoch   9 |  1600/ 1863 batches | lr 3.15 | ms/batch 13.76 | loss  5.48 | ppl   238.76\n",
      "| epoch   9 |  1800/ 1863 batches | lr 3.15 | ms/batch 13.81 | loss  5.50 | ppl   243.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 27.07s | valid loss  5.68 | valid ppl   293.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 1863 batches | lr 2.99 | ms/batch 13.96 | loss  5.49 | ppl   241.37\n",
      "| epoch  10 |   400/ 1863 batches | lr 2.99 | ms/batch 13.80 | loss  5.46 | ppl   234.88\n",
      "| epoch  10 |   600/ 1863 batches | lr 2.99 | ms/batch 13.79 | loss  5.44 | ppl   230.54\n",
      "| epoch  10 |   800/ 1863 batches | lr 2.99 | ms/batch 13.82 | loss  5.46 | ppl   235.11\n",
      "| epoch  10 |  1000/ 1863 batches | lr 2.99 | ms/batch 14.99 | loss  5.48 | ppl   238.85\n",
      "| epoch  10 |  1200/ 1863 batches | lr 2.99 | ms/batch 14.78 | loss  5.48 | ppl   238.92\n",
      "| epoch  10 |  1400/ 1863 batches | lr 2.99 | ms/batch 15.12 | loss  5.41 | ppl   222.93\n",
      "| epoch  10 |  1600/ 1863 batches | lr 2.99 | ms/batch 14.64 | loss  5.42 | ppl   225.63\n",
      "| epoch  10 |  1800/ 1863 batches | lr 2.99 | ms/batch 13.98 | loss  5.45 | ppl   232.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 27.89s | valid loss  5.68 | valid ppl   291.84\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n",
    "torch.save(best_model.state_dict(), 'models/Transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.58 | test ppl   266.23\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "best_model_loaded = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "best_model_loaded.load_state_dict(torch.load('models/Transformer.pth'), strict=True)\n",
    "test_loss = evaluate(best_model_loaded, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "td{font-size: 12px}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "td{font-size: 12px}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on each hyper-parameter.\n",
    "\n",
    "The experiment results for each parameter set are summarized as table below.\n",
    "\n",
    "| emsize | nhid | nlayers | nhead | dropout | test_loss |\n",
    "|--------|------|---------|-------|---------|-----------|\n",
    "| 256    | 128  | 2       | 2     | 0.5     | 5.58      |\n",
    "|        |      |         |       |         |           |\n",
    "|        |      |         |       |         |           |\n",
    "|        |      |         |       |         |           |\n",
    "|        |      |         |       |         |           |\n",
    "|        |      |         |       |         |           |\n",
    "|        |      |         |       |         |           |\n",
    "|        |      |         |       |         |           |\n",
    "|        |      |         |       |         |           |\n",
    "|        |      |         |       |         |           |\n",
    "|        |      |         |       |         |           |\n",
    "|        |      |         |       |         |           |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning-20] *",
   "language": "python",
   "name": "conda-env-deep-learning-20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
