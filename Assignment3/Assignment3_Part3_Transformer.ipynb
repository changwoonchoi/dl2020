{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 3: Transformer\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Jeonghee Jo, October 2019, and modified by Jungbeom Lee, October 2020.\n",
    "\n",
    "This is about Transformer (Vaswani et al., 2017).\n",
    "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n",
    "\n",
    "Original blog post & code:\n",
    "https://github.com/Kyubyong/transformer (Tensorflow)\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html (PyTorch)\n",
    "\n",
    "\n",
    "That said, you are allowed to copy paste the codes from the original repo.\n",
    "HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n",
    "You will learn a lot while wrapping around your head during the implementation. And you will understand nuts and bolts of Transformers more clearly in a code level.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1-3**, run the *CollectSubmission.sh* script with your **Student number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your student number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* 20xx-xxxxx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The Grading is as follows:\n",
    "\n",
    "This assignment is an on/off one: just make this notebook **\"work\"** without problem by: \n",
    "\n",
    "1. Train your model using at least <font color=red> 12 different hyperparameter set </font>. Report performance results computed in the last code block <font color=red> for corresponding each hyperparameter set </font>. Plus, <font color=red> submit the one checkpoint file </font> of your best model. \n",
    "\n",
    "2. Please provide the analysis of changed hyper-parameters. (10 points)\n",
    "\n",
    "The details are described in <font color=red>**transformer_modules.py**</font>. (There is nothing to implement in this notebook.)\n",
    "\n",
    "\n",
    "Now proceed to the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change hyper-parameters in this code block!\n",
    "\n",
    "\n",
    "emsize = 512 # embedding dimension\n",
    "nhid = 512 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "batch_size = 32\n",
    "eval_batch_size = 16\n",
    "epochs = 10 # The number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# pip install torchtext==0.6.0\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_modules import *\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)\n",
    "\n",
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1863 batches | lr 5.00 | ms/batch 28.43 | loss  9.07 | ppl  8683.99\n",
      "| epoch   1 |   400/ 1863 batches | lr 5.00 | ms/batch 28.33 | loss  7.21 | ppl  1356.57\n",
      "| epoch   1 |   600/ 1863 batches | lr 5.00 | ms/batch 28.55 | loss  6.58 | ppl   722.39\n",
      "| epoch   1 |   800/ 1863 batches | lr 5.00 | ms/batch 28.65 | loss  6.33 | ppl   563.24\n",
      "| epoch   1 |  1000/ 1863 batches | lr 5.00 | ms/batch 28.96 | loss  6.19 | ppl   489.16\n",
      "| epoch   1 |  1200/ 1863 batches | lr 5.00 | ms/batch 29.33 | loss  6.06 | ppl   428.93\n",
      "| epoch   1 |  1400/ 1863 batches | lr 5.00 | ms/batch 30.50 | loss  5.94 | ppl   380.54\n",
      "| epoch   1 |  1600/ 1863 batches | lr 5.00 | ms/batch 29.74 | loss  5.87 | ppl   354.29\n",
      "| epoch   1 |  1800/ 1863 batches | lr 5.00 | ms/batch 29.84 | loss  5.85 | ppl   348.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 56.54s | valid loss  5.69 | valid ppl   295.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1863 batches | lr 4.51 | ms/batch 29.51 | loss  5.79 | ppl   326.06\n",
      "| epoch   2 |   400/ 1863 batches | lr 4.51 | ms/batch 30.04 | loss  5.70 | ppl   299.99\n",
      "| epoch   2 |   600/ 1863 batches | lr 4.51 | ms/batch 30.54 | loss  5.61 | ppl   272.14\n",
      "| epoch   2 |   800/ 1863 batches | lr 4.51 | ms/batch 30.04 | loss  5.60 | ppl   270.24\n",
      "| epoch   2 |  1000/ 1863 batches | lr 4.51 | ms/batch 30.06 | loss  5.58 | ppl   265.81\n",
      "| epoch   2 |  1200/ 1863 batches | lr 4.51 | ms/batch 30.48 | loss  5.56 | ppl   259.00\n",
      "| epoch   2 |  1400/ 1863 batches | lr 4.51 | ms/batch 29.96 | loss  5.45 | ppl   232.59\n",
      "| epoch   2 |  1600/ 1863 batches | lr 4.51 | ms/batch 29.54 | loss  5.43 | ppl   227.12\n",
      "| epoch   2 |  1800/ 1863 batches | lr 4.51 | ms/batch 29.48 | loss  5.45 | ppl   233.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 57.84s | valid loss  5.43 | valid ppl   227.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1863 batches | lr 4.29 | ms/batch 29.76 | loss  5.43 | ppl   227.29\n",
      "| epoch   3 |   400/ 1863 batches | lr 4.29 | ms/batch 30.04 | loss  5.37 | ppl   214.84\n",
      "| epoch   3 |   600/ 1863 batches | lr 4.29 | ms/batch 29.97 | loss  5.31 | ppl   201.69\n",
      "| epoch   3 |   800/ 1863 batches | lr 4.29 | ms/batch 29.21 | loss  5.32 | ppl   203.64\n",
      "| epoch   3 |  1000/ 1863 batches | lr 4.29 | ms/batch 29.59 | loss  5.31 | ppl   203.32\n",
      "| epoch   3 |  1200/ 1863 batches | lr 4.29 | ms/batch 29.42 | loss  5.30 | ppl   200.36\n",
      "| epoch   3 |  1400/ 1863 batches | lr 4.29 | ms/batch 29.41 | loss  5.18 | ppl   178.49\n",
      "| epoch   3 |  1600/ 1863 batches | lr 4.29 | ms/batch 29.54 | loss  5.18 | ppl   177.53\n",
      "| epoch   3 |  1800/ 1863 batches | lr 4.29 | ms/batch 29.42 | loss  5.21 | ppl   183.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 57.19s | valid loss  5.29 | valid ppl   199.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 1863 batches | lr 4.07 | ms/batch 29.64 | loss  5.20 | ppl   182.13\n",
      "| epoch   4 |   400/ 1863 batches | lr 4.07 | ms/batch 29.56 | loss  5.15 | ppl   171.72\n",
      "| epoch   4 |   600/ 1863 batches | lr 4.07 | ms/batch 29.56 | loss  5.11 | ppl   165.35\n",
      "| epoch   4 |   800/ 1863 batches | lr 4.07 | ms/batch 29.37 | loss  5.13 | ppl   168.32\n",
      "| epoch   4 |  1000/ 1863 batches | lr 4.07 | ms/batch 29.44 | loss  5.13 | ppl   168.24\n",
      "| epoch   4 |  1200/ 1863 batches | lr 4.07 | ms/batch 29.87 | loss  5.12 | ppl   167.32\n",
      "| epoch   4 |  1400/ 1863 batches | lr 4.07 | ms/batch 29.62 | loss  5.00 | ppl   149.10\n",
      "| epoch   4 |  1600/ 1863 batches | lr 4.07 | ms/batch 29.39 | loss  5.01 | ppl   149.24\n",
      "| epoch   4 |  1800/ 1863 batches | lr 4.07 | ms/batch 29.27 | loss  5.05 | ppl   155.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 57.02s | valid loss  5.23 | valid ppl   187.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 1863 batches | lr 3.87 | ms/batch 29.45 | loss  5.04 | ppl   154.15\n",
      "| epoch   5 |   400/ 1863 batches | lr 3.87 | ms/batch 29.37 | loss  4.98 | ppl   146.01\n",
      "| epoch   5 |   600/ 1863 batches | lr 3.87 | ms/batch 29.33 | loss  4.96 | ppl   142.17\n",
      "| epoch   5 |   800/ 1863 batches | lr 3.87 | ms/batch 29.35 | loss  4.98 | ppl   145.73\n",
      "| epoch   5 |  1000/ 1863 batches | lr 3.87 | ms/batch 29.25 | loss  4.98 | ppl   145.74\n",
      "| epoch   5 |  1200/ 1863 batches | lr 3.87 | ms/batch 29.34 | loss  4.98 | ppl   146.14\n",
      "| epoch   5 |  1400/ 1863 batches | lr 3.87 | ms/batch 29.20 | loss  4.86 | ppl   129.65\n",
      "| epoch   5 |  1600/ 1863 batches | lr 3.87 | ms/batch 29.34 | loss  4.87 | ppl   130.22\n",
      "| epoch   5 |  1800/ 1863 batches | lr 3.87 | ms/batch 29.31 | loss  4.91 | ppl   135.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 56.65s | valid loss  5.15 | valid ppl   173.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 1863 batches | lr 3.68 | ms/batch 29.40 | loss  4.91 | ppl   135.01\n",
      "| epoch   6 |   400/ 1863 batches | lr 3.68 | ms/batch 29.11 | loss  4.85 | ppl   127.19\n",
      "| epoch   6 |   600/ 1863 batches | lr 3.68 | ms/batch 29.09 | loss  4.83 | ppl   125.69\n",
      "| epoch   6 |   800/ 1863 batches | lr 3.68 | ms/batch 29.11 | loss  4.86 | ppl   128.41\n",
      "| epoch   6 |  1000/ 1863 batches | lr 3.68 | ms/batch 29.06 | loss  4.86 | ppl   128.53\n",
      "| epoch   6 |  1200/ 1863 batches | lr 3.68 | ms/batch 29.04 | loss  4.86 | ppl   129.01\n",
      "| epoch   6 |  1400/ 1863 batches | lr 3.68 | ms/batch 29.16 | loss  4.74 | ppl   114.57\n",
      "| epoch   6 |  1600/ 1863 batches | lr 3.68 | ms/batch 29.06 | loss  4.75 | ppl   115.64\n",
      "| epoch   6 |  1800/ 1863 batches | lr 3.68 | ms/batch 29.93 | loss  4.79 | ppl   120.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 56.50s | valid loss  5.14 | valid ppl   171.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 1863 batches | lr 3.49 | ms/batch 29.37 | loss  4.79 | ppl   120.33\n",
      "| epoch   7 |   400/ 1863 batches | lr 3.49 | ms/batch 29.21 | loss  4.73 | ppl   113.59\n",
      "| epoch   7 |   600/ 1863 batches | lr 3.49 | ms/batch 29.18 | loss  4.72 | ppl   112.20\n",
      "| epoch   7 |   800/ 1863 batches | lr 3.49 | ms/batch 29.22 | loss  4.75 | ppl   115.66\n",
      "| epoch   7 |  1000/ 1863 batches | lr 3.49 | ms/batch 29.30 | loss  4.75 | ppl   115.49\n",
      "| epoch   7 |  1200/ 1863 batches | lr 3.49 | ms/batch 29.20 | loss  4.76 | ppl   116.25\n",
      "| epoch   7 |  1400/ 1863 batches | lr 3.49 | ms/batch 29.28 | loss  4.64 | ppl   103.03\n",
      "| epoch   7 |  1600/ 1863 batches | lr 3.49 | ms/batch 29.23 | loss  4.65 | ppl   104.49\n",
      "| epoch   7 |  1800/ 1863 batches | lr 3.49 | ms/batch 29.15 | loss  4.68 | ppl   108.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 56.49s | valid loss  5.15 | valid ppl   172.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 1863 batches | lr 3.32 | ms/batch 29.18 | loss  4.69 | ppl   108.94\n",
      "| epoch   8 |   400/ 1863 batches | lr 3.32 | ms/batch 29.06 | loss  4.63 | ppl   102.76\n",
      "| epoch   8 |   600/ 1863 batches | lr 3.32 | ms/batch 29.22 | loss  4.63 | ppl   102.11\n",
      "| epoch   8 |   800/ 1863 batches | lr 3.32 | ms/batch 29.17 | loss  4.65 | ppl   104.77\n",
      "| epoch   8 |  1000/ 1863 batches | lr 3.32 | ms/batch 29.11 | loss  4.66 | ppl   105.51\n",
      "| epoch   8 |  1200/ 1863 batches | lr 3.32 | ms/batch 29.09 | loss  4.66 | ppl   105.97\n",
      "| epoch   8 |  1400/ 1863 batches | lr 3.32 | ms/batch 29.06 | loss  4.55 | ppl    94.25\n",
      "| epoch   8 |  1600/ 1863 batches | lr 3.32 | ms/batch 29.21 | loss  4.56 | ppl    95.62\n",
      "| epoch   8 |  1800/ 1863 batches | lr 3.32 | ms/batch 29.10 | loss  4.59 | ppl    98.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 56.29s | valid loss  5.10 | valid ppl   164.55\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   9 |   200/ 1863 batches | lr 3.15 | ms/batch 29.19 | loss  4.60 | ppl    99.34\n",
      "| epoch   9 |   400/ 1863 batches | lr 3.15 | ms/batch 29.21 | loss  4.54 | ppl    93.60\n",
      "| epoch   9 |   600/ 1863 batches | lr 3.15 | ms/batch 29.28 | loss  4.54 | ppl    93.74\n",
      "| epoch   9 |   800/ 1863 batches | lr 3.15 | ms/batch 29.24 | loss  4.57 | ppl    96.48\n",
      "| epoch   9 |  1000/ 1863 batches | lr 3.15 | ms/batch 29.14 | loss  4.57 | ppl    96.55\n",
      "| epoch   9 |  1200/ 1863 batches | lr 3.15 | ms/batch 29.31 | loss  4.58 | ppl    97.34\n",
      "| epoch   9 |  1400/ 1863 batches | lr 3.15 | ms/batch 29.18 | loss  4.46 | ppl    86.46\n",
      "| epoch   9 |  1600/ 1863 batches | lr 3.15 | ms/batch 29.12 | loss  4.48 | ppl    87.82\n",
      "| epoch   9 |  1800/ 1863 batches | lr 3.15 | ms/batch 29.24 | loss  4.51 | ppl    90.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 56.46s | valid loss  5.12 | valid ppl   167.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 1863 batches | lr 2.99 | ms/batch 29.33 | loss  4.52 | ppl    91.53\n",
      "| epoch  10 |   400/ 1863 batches | lr 2.99 | ms/batch 29.26 | loss  4.46 | ppl    86.31\n",
      "| epoch  10 |   600/ 1863 batches | lr 2.99 | ms/batch 29.26 | loss  4.47 | ppl    86.95\n",
      "| epoch  10 |   800/ 1863 batches | lr 2.99 | ms/batch 29.25 | loss  4.49 | ppl    88.88\n",
      "| epoch  10 |  1000/ 1863 batches | lr 2.99 | ms/batch 29.25 | loss  4.49 | ppl    89.21\n",
      "| epoch  10 |  1200/ 1863 batches | lr 2.99 | ms/batch 29.26 | loss  4.50 | ppl    89.69\n",
      "| epoch  10 |  1400/ 1863 batches | lr 2.99 | ms/batch 29.26 | loss  4.38 | ppl    80.17\n",
      "| epoch  10 |  1600/ 1863 batches | lr 2.99 | ms/batch 29.26 | loss  4.40 | ppl    81.65\n",
      "| epoch  10 |  1800/ 1863 batches | lr 2.99 | ms/batch 29.28 | loss  4.43 | ppl    84.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 56.55s | valid loss  5.12 | valid ppl   166.79\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n",
    "torch.save(best_model.state_dict(), 'models/Transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.05 | test ppl   155.74\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "best_model_loaded = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "best_model_loaded.load_state_dict(torch.load('models/Transformer.pth'), strict=True)\n",
    "test_loss = evaluate(best_model_loaded, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "td{font-size: 12px}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "td{font-size: 12px}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on each hyper-parameter.\n",
    "\n",
    "The experiment results for each parameter set are summarized as table below.\n",
    "\n",
    "|exp#| emsize | nhid | nlayers | nhead | dropout | test_loss |\n",
    "|----|--------|------|---------|-------|---------|-----------|\n",
    "|  1 | 256    | 128  | 2       | 2     | 0.5     | 5.58      |\n",
    "|  2 | 256    | 128  | 2       | 2     | 0.2     | 5.34      |\n",
    "|  3 | 256    | 128  | 2       | 4     | 0.2     | 5.32      |\n",
    "|  4 | 256    | 256  | 2       | 4     | 0.2     | 5.26      |\n",
    "|  5 | 256    | 256  | 4       | 4     | 0.2     | 5.11      |\n",
    "|  6 | 256    | 256  | 8       | 4     | 0.2     | 6.93      |\n",
    "|  7 | 256    | 256  | 4       | 8     | 0.2     | 5.10      |\n",
    "|  8 | 256    | 512  | 4       | 8     | 0.2     | 5.05      |\n",
    "|  9 | 256    | 512  | 4       | 8     | 0.3     | 5.14      |\n",
    "| 10 | 256    | 512  | 4       | 8     | 0.1     | 5.07      |\n",
    "| 11 | 270    | 512  | 6       | 8     | 0.1     | 5.10      |\n",
    "| 12 | 512    | 512  | 4       | 8     | 0.1     | 5.09      |\n",
    "| 13 | 512    | 512  | 4       | 8     | 0.2     | 5.05      |\n",
    "\n",
    "As shown in table above, one can observe that drop out rate 0.5 is too big for our model. (Since I ran code for only 10 epoch, high drop out rate makes thd model be underfitted.) Also, with emsize 256 and hid size 256, nlayer of 8 can't converge. (The experiment number 6.) Generally, the higher emsize, the higher nhid and the higher nhead lead to the better test_loss. But with epoch 10, we can observe that too deep or too big model couldn't converge enough. The chekpoint of best model which shows the best test_loss is attached in submission zip file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep-learning-20] *",
   "language": "python",
   "name": "conda-env-deep-learning-20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
